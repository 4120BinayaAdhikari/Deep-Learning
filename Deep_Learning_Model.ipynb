{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network on Concrete Dataset\n",
    "\n",
    "The concrete dataset contains measurements of various properties of concrete mixtures and the strength of the concrete produced from those mixtures. The goal of the neural network is to predict the strength of concrete based on these properties. The dataset contains 1030 samples and 8 input features, including the amount of cement, slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate, and age of the concrete.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement   slag    ash  water  superplastic  coarseagg  fineagg  age  \\\n",
      "0   141.3  212.0    0.0  203.5           0.0      971.8    748.5   28   \n",
      "1   168.9   42.2  124.3  158.3          10.8     1080.8    796.2   14   \n",
      "2   250.0    0.0   95.7  187.4           5.5      956.9    861.2   28   \n",
      "3   266.0  114.0    0.0  228.0           0.0      932.0    670.0   28   \n",
      "4   154.8  183.4    0.0  193.3           9.1     1047.4    696.7   28   \n",
      "\n",
      "   strength  \n",
      "0     29.89  \n",
      "1     23.51  \n",
      "2     29.22  \n",
      "3     45.85  \n",
      "4     18.29  \n"
     ]
    }
   ],
   "source": [
    "# Loading Data Files\n",
    "concrete = pd.read_csv(r\"C:\\MachineLearning\\Deep_Learning_ConcreteData\\concrete.csv\")\n",
    "print(concrete.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the values by scaling them proportionally within the range [0, 1] and applying normalization to a dataframe using the apply method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##custom normalization function\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply normalization to entire data frame\n",
    "concrete_norm = concrete.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1030.000000\n",
      "mean        0.417191\n",
      "std         0.208119\n",
      "min         0.000000\n",
      "25%         0.266351\n",
      "50%         0.400087\n",
      "75%         0.545721\n",
      "max         1.000000\n",
      "Name: strength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#confirm that the range is now between zero and one\n",
    "print(concrete_norm['strength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1030.000000\n",
      "mean       35.817961\n",
      "std        16.705742\n",
      "min         2.330000\n",
      "25%        23.710000\n",
      "50%        34.445000\n",
      "75%        46.135000\n",
      "max        82.600000\n",
      "Name: strength, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "'compared to the original minimum and maximum'\n",
    "print(concrete['strength'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training and test data\n",
    "concrete_train = concrete_norm.iloc[:773, :]\n",
    "concrete_test = concrete_norm.iloc[773:1030, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a simple neural network model with one hidden layer and an output layer for regression tasks. \n",
    "The model uses the ReLU activation function in the hidden layer and linear activation in the output layer. It is compiled with the mean squared error loss function and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def neuralnet():\n",
    "    # Define a simple neural network with one hidden layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation='relu', input_dim=8))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    # Compile the model with a mean squared error loss function and an optimizer\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code below defines a function that builds and trains a neural network model based on a given formula and dataset. The model has one hidden layer with a customizable number of neurons, uses ReLU activation in the hidden layer, linear activation in the output layer, and is trained using mean squared error loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnet(formula, data, hidden):\n",
    "    # Parse the formula string to extract the response and predictor variables\n",
    "    response, predictors = dmatrices(formula, data)\n",
    "\n",
    "    # Create a sequential model with one hidden layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hidden, activation='relu', input_dim=predictors.shape[1]))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    # Compile the model with a mean squared error loss function and an optimizer\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model on the predictor and response variables\n",
    "    model.fit(predictors, response, epochs=100, batch_size=10)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a simple Artificial Neural Network (ANN) model using the neuralnet package in R. The neuralnet function takes a formula and a dataset as inputs and trains a neural network model based on the provided data\n",
    "\n",
    "The hidden parameter in the neuralnet function specifies the number of hidden neurons in the neural network. In this case, we are using only a single hidden neuron, which means that the model is relatively simple and may not be able to capture complex relationships between the predictors and the target variable.\n",
    "\n",
    "By training this simple ANN model, we can compare its performance with the other machine learning models we have used on the same dataset, such as the Random Forest and XGBoost models. We can then determine which model provides the best predictive accuracy for the concrete strength dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "78/78 [==============================] - 1s 2ms/step - loss: 0.1911\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1385\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.1037\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0800\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0647\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0554\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0499\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0454\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0446\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 62/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0441\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0441\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.0441\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.0441\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0441\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.0441\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0441\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0441\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "# Simple ANN with only a single hidden neuron\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "np.random.seed(12345) # to guarantee repeatable results\n",
    "concrete_model = neuralnet(formula = \"strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age\",\n",
    "data = concrete_train, hidden = 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can observe on how it improved the performance of the model by increasing the number of hidden layers, the number of neurons in each layer, and adding other advanced techniques like dropout and batch normalization.\n",
    "\n",
    "Here's an example of a more complex architecture with two hidden layers, 64 and 32 neurons respectively, and dropout and batch normalization layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 2s 2ms/step - loss: 1.4293\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.9347\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6551\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6458\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5006\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4676\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4175\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3437\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3438\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3227\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2962\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2703\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2450\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2291\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2107\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2256\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1707\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1726\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1503\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1439\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1420\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1188\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0968\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0883\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0980\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0802\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.0708\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0661\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0720\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0629\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0570\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0560\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0529\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0454\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.0459\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0435\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0414\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0336\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0383\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0331\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0382\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0317\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0316\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0288\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0296\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0284\n",
      "Test loss: 0.011509106494486332\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Prediction: [0.48274022] Actual: 0.4604459947676592\n",
      "Prediction: [0.40026683] Actual: 0.4522237448610939\n",
      "Prediction: [0.45731598] Actual: 0.5137660396162951\n",
      "Prediction: [0.61192757] Actual: 0.4107387566961505\n",
      "Prediction: [0.36257684] Actual: 0.4623146879282422\n"
     ]
    }
   ],
   "source": [
    "# Split into predictors and target\n",
    "predictors = concrete_norm.iloc[:,:-1]\n",
    "target = concrete_norm.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize predictors\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(8,), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate model on test set\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print some predictions and actual values\n",
    "for i in range(5):\n",
    "    print('Prediction:', y_pred[i], 'Actual:', y_test.iloc[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture has two hidden layers with 64 and 32 neurons respectively. Both hidden layers include batch normalization layers and dropout layers to prevent overfitting. The output layer has a single neuron with a linear activation function to predict the concrete strength.\n",
    "\n",
    "I also added batch normalization layers after each hidden layer to help with internal covariate shift, which is a common problem in deep neural networks. Batch normalization normalizes the inputs to a layer for each batch, helping to stabilize the learning process and reduce overfitting.\n",
    "\n",
    "Finally, I used a linear activation function for the output layer since we are predicting a continuous variable (concrete strength).\n",
    "\n",
    "By using a more complex architecture with batch normalization and dropout layers, I improved the performance of the neural network on the concrete dataset.\n",
    "\n",
    "The model aims to minimize the difference between its predictions and the actual values.\n",
    "\n",
    "Interpreting the result:\n",
    "\n",
    "The decreasing loss during training indicates that the model is gradually improving its performance on the training data.\n",
    "The predictions shown are examples of how the model performs on unseen data. By comparing the predicted values with the actual values, you can assess the model's accuracy. In this case, the predictions are relatively close to the actual values, but the exact interpretation would depend on the specific context and problem being solved.\n",
    "To further evaluate the model's performance, we can use additional evaluation metrics, such as mean absolute error (MAE) or root mean squared error (RMSE), and assess its performance on a separate validation or test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6959ce72daee32acd7e6f1b5e318ba8d57f6fe2ad9643e51a4f02ee5e08d6032"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
